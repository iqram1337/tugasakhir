{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d6c819",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/seisbench/seisbench/blob/additional_example_workflows/examples/03a_training_phasenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241827c2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image](https://raw.githubusercontent.com/seisbench/seisbench/main/docs/_static/seisbench_logo_subtitle_outlined.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95f074",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*This code is necessary on colab to install SeisBench. If SeisBench is already installed on your machine, you can skip this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13e49f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install seisbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e4590",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*This cell is required to circumvent an issue with colab and obspy. For details, check this issue in the obspy documentation: https://github.com/obspy/obspy/issues/2547*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77bdb48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     import obspy\n",
    "#     obspy.read()\n",
    "# except TypeError:\n",
    "#     # Needs to restart the runtime once, because obspy only works properly after restart.\n",
    "#     print('Stopping RUNTIME. If you run this code for the first time, this is expected. Colaboratory will restart automatically. Please run again.')\n",
    "#     exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f032984",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training PhaseNet\n",
    "\n",
    "This tutorial shows how to train a model with SeisBench, using PhaseNet as an example. This brings together the three main components of SeisBench: data, models and generate.\n",
    "\n",
    "The tutorial is intended to highlight the basic principles of training models in SeisBench. However, this will not necessarily be best practice for more elaborate experiments. As a reference how to set up larger studies and which augmentations can be used for which models, we refer to the implementation of our pick benchmark at [https://github.com/seisbench/pick-benchmark](https://github.com/seisbench/pick-benchmark).\n",
    "\n",
    "*Note: As this tutorial brings together different parts of seisbench, it is recommended to go through the basic tutorials first before beginning this tutorial. In addition, this tutorial assumes some familiarity with pytorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689c1ea9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9649cd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model and data\n",
    "\n",
    "We create a randomly initialized PhaseNet model using `seisbench.models`. If available, you can move your model onto the GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b596d33f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = sbm.PhaseNet(phases=\"PSN\", norm=\"peak\")\n",
    "\n",
    "# model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd83fdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As training data we use the ETHZ dataset. Note that we set the sampling rate to 100 Hz to ensure that all examples are consistent in terms of sampling rate. We split the data into training, development and test sets according to the splits provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbbf8d17",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 12:55:05,619 | seisbench | WARNING | Check available storage and memory before downloading and general use of Iquique dataset. Dataset size: waveforms.hdf5 ~5Gb, metadata.csv ~2.6Mb\n",
      "2024-02-05 12:55:07,691 | seisbench | WARNING | Dataset Iquique not in cache.\n",
      "2024-02-05 12:55:07,692 | seisbench | WARNING | Trying to download preprocessed version from SeisBench repository.\n",
      "2024-02-05 12:55:09,738 | seisbench | WARNING | The download precheck failed with status code 404. This is not an error itself, but might indicate a subsequent error. If you encounter an error, this might be caused by the firewall setup of your network. Please check https://github.com/seisbench/seisbench#known-issues for details.\n",
      "2024-02-05 12:55:11,663 | seisbench | WARNING | Dataset Iquique not in SeisBench repository. Starting download and conversion from source.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid method signature",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msbd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIquique\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#train, dev, test = data.train_dev_test()\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\seisbench\\data\\iquique.py:30\u001b[0m, in \u001b[0;36mIquique.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m citation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWoollam, J., Rietbrock, A., Bueno, A. and De Angelis, S., 2019. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvolutional neural network for seismic phase classification, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://doi.org/10.1785/0220180312\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m seisbench\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck available storage and memory before downloading and general use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof Iquique dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset size: waveforms.hdf5 ~5Gb, metadata.csv ~2.6Mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(citation\u001b[38;5;241m=\u001b[39mcitation, repository_lookup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\seisbench\\data\\base.py:2173\u001b[0m, in \u001b[0;36mBenchmarkDataset.__init__\u001b[1;34m(self, chunks, citation, license, force, wait_for_file, repository_lookup, download_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2167\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dataset(writer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtmp_download_args)\n\u001b[0;32m   2169\u001b[0m     files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2170\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaveforms\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2172\u001b[0m     ]\n\u001b[1;32m-> 2173\u001b[0m     \u001b[43mseisbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_if_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_file\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_internal(), chunks\u001b[38;5;241m=\u001b[39mchunks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\seisbench\\util\\file.py:259\u001b[0m, in \u001b[0;36mcallback_if_uncached\u001b[1;34m(files, callback, force, wait_for_file, test_interval)\u001b[0m\n\u001b[0;32m    257\u001b[0m         callback(partial_files[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m partial_file \u001b[38;5;129;01min\u001b[39;00m partial_files:\n",
      "File \u001b[1;32mD:\\miniconda3\\lib\\site-packages\\seisbench\\data\\base.py:2153\u001b[0m, in \u001b[0;36mBenchmarkDataset.__init__.<locals>.download_callback\u001b[1;34m(files)\u001b[0m\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m successful_repository_download:\n\u001b[0;32m   2148\u001b[0m     seisbench\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   2149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in SeisBench repository. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting download and conversion from source.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2151\u001b[0m     )\n\u001b[1;32m-> 2153\u001b[0m     download_dataset_parameters \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_dataset\u001b[49m\n\u001b[0;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m   2156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m download_dataset_parameters \u001b[38;5;129;01mand\u001b[39;00m chunk \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2158\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData set seems not to support chunking, but chunk provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2159\u001b[0m         )\n",
      "File \u001b[1;32mD:\\miniconda3\\lib\\inspect.py:3113\u001b[0m, in \u001b[0;36msignature\u001b[1;34m(obj, follow_wrapped)\u001b[0m\n\u001b[0;32m   3111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   3112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_wrapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda3\\lib\\inspect.py:2862\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[1;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[0;32m   2859\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   2861\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2863\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda3\\lib\\inspect.py:2269\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[1;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[0;32m   2266\u001b[0m sig \u001b[38;5;241m=\u001b[39m _get_signature_of(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m)\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_bound_arg:\n\u001b[1;32m-> 2269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_bound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sig\n",
      "File \u001b[1;32mD:\\miniconda3\\lib\\inspect.py:1848\u001b[0m, in \u001b[0;36m_signature_bound_method\u001b[1;34m(sig)\u001b[0m\n\u001b[0;32m   1845\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m   1847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params \u001b[38;5;129;01mor\u001b[39;00m params[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m-> 1848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid method signature\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1850\u001b[0m kind \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkind\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;129;01min\u001b[39;00m (_POSITIONAL_OR_KEYWORD, _POSITIONAL_ONLY):\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;66;03m# Drop first parameter:\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m     \u001b[38;5;66;03m# '(p1, p2[, ...])' -> '(p2[, ...])'\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid method signature"
     ]
    }
   ],
   "source": [
    "data = sbd.Iquique(sampling_rate=100)\n",
    "#train, dev, test = data.train_dev_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876fa35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c037b8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The ETHZ dataset contains detailed labels for the phases. However, for this example we only want to differentiate between P and S picks. Therefore, we define a dictionary mapping the detailed picks to their phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b8ed6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phase_dict = {\n",
    "    \"trace_p_arrival_sample\": \"P\",\n",
    "    \"trace_pP_arrival_sample\": \"P\",\n",
    "    \"trace_P_arrival_sample\": \"P\",\n",
    "    \"trace_P1_arrival_sample\": \"P\",\n",
    "    \"trace_Pg_arrival_sample\": \"P\",\n",
    "    \"trace_Pn_arrival_sample\": \"P\",\n",
    "    \"trace_PmP_arrival_sample\": \"P\",\n",
    "    \"trace_pwP_arrival_sample\": \"P\",\n",
    "    \"trace_pwPm_arrival_sample\": \"P\",\n",
    "    \"trace_s_arrival_sample\": \"S\",\n",
    "    \"trace_S_arrival_sample\": \"S\",\n",
    "    \"trace_S1_arrival_sample\": \"S\",\n",
    "    \"trace_Sg_arrival_sample\": \"S\",\n",
    "    \"trace_SmS_arrival_sample\": \"S\",\n",
    "    \"trace_Sn_arrival_sample\": \"S\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7cfc9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we define two generators with identical augmentations, one for training, one for validation. The augmentations are:\n",
    "1. Selection of a (long) window around a pick. This way, we ensure that out data always contains a pick.\n",
    "1. Selection of a random window with 3001 samples, the input length of PhaseNet.\n",
    "1. A normalization, consisting of demeaning and amplitude normalization.\n",
    "1. A change of datatype to float32, as this is expected by the pytorch model.\n",
    "1. A probabilistic label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce52377",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m sbg\u001b[38;5;241m.\u001b[39mGenericGenerator(\u001b[43mtrain\u001b[49m)\n\u001b[0;32m      2\u001b[0m dev_generator \u001b[38;5;241m=\u001b[39m sbg\u001b[38;5;241m.\u001b[39mGenericGenerator(dev)\n\u001b[0;32m      4\u001b[0m augmentations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     sbg\u001b[38;5;241m.\u001b[39mWindowAroundSample(\u001b[38;5;28mlist\u001b[39m(phase_dict\u001b[38;5;241m.\u001b[39mkeys()), samples_before\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, windowlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6000\u001b[39m, selection\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m     sbg\u001b[38;5;241m.\u001b[39mRandomWindow(windowlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3001\u001b[39m, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     sbg\u001b[38;5;241m.\u001b[39mProbabilisticLabeller(label_columns\u001b[38;5;241m=\u001b[39mphase_dict, sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
    "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
    "    sbg.Normalize(demean_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(label_columns=phase_dict, sigma=30, dim=0)\n",
    "]\n",
    "\n",
    "train_generator.add_augmentations(augmentations)\n",
    "dev_generator.add_augmentations(augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e75a3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's visualize a few training examples. Everytime you run the cell below, you'll see a different training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef0570",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = train_generator[np.random.randint(len(train_generator))]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "axs = fig.subplots(2, 1, sharex=True, gridspec_kw={\"hspace\": 0, \"height_ratios\": [3, 1]})\n",
    "axs[0].plot(sample[\"X\"].T)\n",
    "axs[1].plot(sample[\"y\"].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005455e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SeisBench generators are pytorch datasets. Therefore, we can pass them to pytorch data loaders. These will automatically take care of parallel loading and batching. Here we create one loader for training and one for validation. We choose a batch size of 256 samples. This batch size should fit on most hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2e42c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4  # The number of threads used for loading data\n",
    "\n",
    "train_loader = DataLoader(train_generator, batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding)\n",
    "dev_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b009a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024bf3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we got all components for training the model. What we still need to do is define the optimizer and the loss, and write the training and validation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5581f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "epochs = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def loss_fn(y_pred, y_true, eps=1e-5):\n",
    "    # vector cross entropy loss\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h = h.mean(-1).sum(-1)  # Mean along sample dimension and sum along pick dimension\n",
    "    h = h.mean()  # Mean over batch axis\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e5e96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(batch[\"X\"].to(model.device))\n",
    "        loss = loss_fn(pred, batch[\"y\"].to(model.device))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_id % 5 == 0:\n",
    "            loss, current = loss.item(), batch_id * batch[\"X\"].shape[0]\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    model.eval()  # close the model for evaluation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            pred = model(batch[\"X\"].to(model.device))\n",
    "            test_loss += loss_fn(pred, batch[\"y\"].to(model.device)).item()\n",
    "\n",
    "    model.train()  # re-open model for training stage\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a1cd8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader)\n",
    "    test_loop(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a43e14",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Not that we trained the model, we can evaluate it. First, we'll check how the model does on an example from the development set. Note that the model will most likely not be fully trained after only five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a38a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = dev_generator[np.random.randint(len(dev_generator))]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "axs = fig.subplots(3, 1, sharex=True, gridspec_kw={\"hspace\": 0, \"height_ratios\": [3, 1, 1]})\n",
    "axs[0].plot(sample[\"X\"].T)\n",
    "axs[1].plot(sample[\"y\"].T)\n",
    "\n",
    "model.eval()  # close the model for evaluation\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(torch.tensor(sample[\"X\"], device=model.device).unsqueeze(0))  # Add a fake batch dimension\n",
    "    pred = pred[0].cpu().numpy()\n",
    "\n",
    "axs[2].plot(pred.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48017ff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As a second option, we'll directly apply our model to an obspy waveform stream using the `annotate` function. For this, we are downloading waveforms through FDSN and annotating them. Note that you could use the `classify` function in a similar fashion.\n",
    "\n",
    "As we trained the model on Swiss data, we use an example event from Switzerland. Note that we deliberately chose a rather easy example, as the model is not fully trained after the low number of epochs. The exact performance of the model will vary depending, because the model training and initialization involves random aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067fcf5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = Client(\"ETH\")\n",
    "\n",
    "t = UTCDateTime(\"2019-11-04T00:59:46.419800Z\")\n",
    "stream = client.get_waveforms(network=\"CH\", station=\"EMING\", location=\"*\", channel=\"HH?\", starttime=t-30, endtime=t+50)\n",
    "\n",
    "annotations = model.annotate(stream)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "axs = fig.subplots(2, 1, sharex=True, gridspec_kw={'hspace': 0})\n",
    "\n",
    "offset = annotations[0].stats.starttime - stream[0].stats.starttime\n",
    "for i in range(3):\n",
    "    axs[0].plot(stream[i].times(), stream[i].data, label=stream[i].stats.channel)\n",
    "    if annotations[i].stats.channel[-1] != \"N\":  # Do not plot noise curve\n",
    "        axs[1].plot(annotations[i].times() + offset, annotations[i].data, label=annotations[i].stats.channel)\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0ef68",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remarks\n",
    "\n",
    "As discussed in the data basics tutorial, loading a SeisBench dataset only means loading the metadata into memory. The waveforms are only loaded once they are requested to save memory. By default, waveforms are **not** cached in memory. For training, this means that the data needs to be read from the file in every epoch again. Depending on your hardware, this will take a lot of time. To solve this issue, you can set the `cache` option, when creating the dataset. Then, all you have to do is call `preload_waveforms` and the data will be loaded into memory and automatically cached. For most practical applications, this option is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055fdfc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
